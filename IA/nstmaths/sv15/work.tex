\documentclass[10pt,oneside,a4paper]{article}

\input{./templatenst/template.tex}

\fancyhead[RO,LE]{{\bfseries NST Maths, SV~15}\\14-05-2022\ 12:00, Video Link}

\begin{document}

\begin{enumerate}

\item 

\begin{align*}
\mathbf{f_1} &= \mathbf{e_1} & \mathbf{f_2} &= \sqrt{2}\mathbf{e_1} + \sqrt{2}\mathbf{e_2}
\end{align*}

\[
\mathbf{x} = 2\mathbf{f_1} + \frac{3}{\sqrt{2}}\mathbf{f_2}
\]

\item 

\begin{enumerate}

\item 

\[
\begin{split}
 & \mathbf{v}\mathbf{u} \\
=& \begin{pmatrix} 1 & 2 & b \\ \end{pmatrix} \begin{pmatrix} 0 \\ 1 \\ 2 \\ \end{pmatrix} \\
=& 0 \times 1 + 1 \times 2 + 2 \times b \\
=& 2 + 2b \\
\end{split}
\]

\item 

\[
\begin{split}
 & \mathbf{u}\mathbf{v} \\
=& \begin{pmatrix} 0 \\ 1 \\ 2 \\ \end{pmatrix} \begin{pmatrix} 1 & 2 & b \\ \end{pmatrix} \\
=& \begin{pmatrix} 0 & 0 & 0 \\ 1 & 2 & b \\ 2 & 4 & 2b \\ \end{pmatrix} \\
\end{split}
\]

\end{enumerate}

\item 

\begin{itemize}

\item $\mathbf{A}^2$

This does not exist: $A$ is a 2x3 matrix, so the dimensions are not consistent for matrix multiplication.

\item $\mathbf{AB}$

\[
\begin{split}
 & \mathbf{AB} \\
=& \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ \end{pmatrix} \begin{pmatrix} 1 & 0 & 1 & 0 \\ 0 & 2 & 0 & 3 \\ 1 & 0 & 2 & 0 \\ \end{pmatrix} \\
=& \begin{pmatrix} 4 & 4 & 7 & 6 \\ 10 & 10 & 16 & 15 \\ \end{pmatrix}
\end{split}
\]

\item $\mathbf{AC}$

This does not exist: $A$ is a 2x3 matrix and $C$ is a 4x2 matrix, so the dimensions are not consistent for matrix multiplication.

\item $\mathbf{CA}$

\[
\begin{split}
 & \mathbf{CA} \\
=& \begin{pmatrix} 1 & 2 \\ 3 & 0 \\ 0 & 1 \\ 1 & 2 \\ \end{pmatrix} \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ \end{pmatrix} \\
=& \begin{pmatrix} 9 & 12 & 15 \\ 3 & 6 & 9 \\ 4 & 5 & 6 \\ 9 & 12 & 15 \\ \end{pmatrix} \\
\end{split}
\]

\item $\mathbf{B}^2$

This does not exist: $B$ is a 3x4 matrix, so the dimensions are not consistent for matrix multiplication.

\item $\mathbf{BC}$

\[
\begin{split}
 & \mathbf{BC} \\
=& \begin{pmatrix} 1 & 0 & 1 & 0 \\ 0 & 2 & 0 & 3 \\ 1 & 0 & 2 & 0 \\ \end{pmatrix} \begin{pmatrix} 1 & 2 \\ 3 & 0 \\ 0 & 1 \\ 1 & 2 \\ \end{pmatrix} \\
=& \begin{pmatrix} 1 & 3 \\ 9 & 6 \\ 1 & 4 \\ \end{pmatrix}
\end{split}
\]

\item $\mathbf{CB}$

This does not exist: $C$ is a 4x2 matrix and $B$ is a 3x4 matrix, so the dimensions are not consistent for matrix multiplication.

\item $\mathbf{C}^2$

This does not exist: $C$ is a 4x2 matrix, so the dimensions are not consistent for matrix multiplication.

\end{itemize}

\item 

\begin{enumerate}

\item $\forall N, M \in \mathbb{Z}^+$

\item $\forall N, M \in \mathbb{Z}^+$

\end{enumerate}

\item 

\[
\begin{split}
 & \mathbf{M}^2 \\
=& \begin{pmatrix} 1 & 2 & 0 \\ 3 & 0 & 1 \\ 1 & 0 & 1 \\ \end{pmatrix} \begin{pmatrix} 1 & 2 & 0 \\ 3 & 0 & 1 \\ 1 & 0 & 1 \\ \end{pmatrix} \\
=& \begin{pmatrix} 7 & 2 & 2 \\ 4 & 6 & 1 \\ 2 & 2 & 1 \\ \end{pmatrix} \\
\end{split}
\]

\[
\begin{split}
 & \mathbf{M}\mathbf{M}^T \\
=& \begin{pmatrix} 1 & 2 & 0 \\ 3 & 0 & 1 \\ 1 & 0 & 1 \\ \end{pmatrix} \begin{pmatrix} 1 & 3 & 1 \\ 2 & 0 & 0 \\ 1 & 0 & 1 \\ \end{pmatrix} \\
=& \begin{pmatrix} 5 & 3 & 1 \\ 3 & 10 & 4 \\ 1 & 4 & 2 \\ \end{pmatrix} \\
\end{split}
\]

\[
\begin{split}
 & \mathbf{M}^T\mathbf{M} \\
=& \begin{pmatrix} 1 & 3 & 1 \\ 2 & 0 & 0 \\ 0 & 1 & 1 \\ \end{pmatrix} \begin{pmatrix} 1 & 2 & 0 \\ 3 & 0 & 1 \\ 1 & 0 & 1 \\ \end{pmatrix} \\
=& \begin{pmatrix} 11 & 2 & 4 \\ 2 & 4 & 0 \\ 4 & 0 & 2 \\ \end{pmatrix} \\
\end{split}
\]

\item 

\[
\mathbf{A} \bullet \mathbf{B} = \mathbf{A} \times \mathbf{B}^T
\]

\[
\begin{split}
((\mathbf{A} \bullet \mathbf{B}) \bullet \mathbf{C})_{ij} &= \sum_k(\mathbf{A} \bullet \mathbf{B})_{ik}(\mathbf{C})_{jk} \\
&= \sum_k\sum_l(\mathbf{A})_{il}(\mathbf{B})_{kl}(\mathbf{C})_{jk} \\
\end{split}
\]

\[
\begin{split}
(\mathbf{A} \bullet (\mathbf{B} \bullet \mathbf{C}))_{ij} &= \sum_k(\mathbf{A})_{ik}(\mathbf{B}\bullet\mathbf{C})_{jk} \\
 &= \sum_k\sum_l(\mathbf{A})_{ik}(\mathbf{B})_{jl}(\mathbf{C})_{kl} \\
 &= \sum_k\sum_l(\mathbf{A})_{il}(\mathbf{C})_{kl}(\mathbf{B})_{jk}
\end{split}
\]

Since the expressions are different, bullet multiplication is not associative.

\item 

\begin{enumerate}

\item 

Consider an arbitrary symmetric matrix $\mathbf{A}$ and an arbitrary antisymmetric matrix $\mathbf{B}$ which have 
the same dimensionality (so they can be multiplied).

\[
\begin{split}
ab_{ij} &= \sum a_{ik}b_{kj} \\
Tr(\mathbf{AB}) &= \sum ab_{ij} \\
Tr(\mathbf{AB}) &= \sum^n_{i=1} \sum^n_{k=1} a_{ik}b_{ki} \\
2Tr(\mathbf{AB}) &= \sum^n_{i=1} \sum^n_{k=1} a_{ik}b_{ki} + \sum^n_{i=1} \sum^n_{k=1} a_{ik}b_{ki} \\
2Tr(\mathbf{AB}) &= \sum^n_{i=1} \sum^n_{k=1} a_{ik}b_{ki} + \sum^n_{i=1} \sum^n_{k=1} a_{ki}b_{ki} \text{ since } \forall i, k \in \mathbb{N}. a_{ik} = a_{ki} \\
2Tr(\mathbf{AB}) &= \sum^n_{i=1} \sum^n_{k=1} a_{ik}b_{ki} - \sum^n_{i=1} \sum^n_{k=1} a_{ki}b_{ik} \text{ since } \forall i, k \in \mathbb{N}. b_{ik} = -b_{ki} \\
2Tr(\mathbf{AB}) &= \sum^n_{i=1} \sum^n_{k=1} a_{ik}b_{ki} - \sum^n_{i=1} \sum^n_{k=1} a_{ik}b_{ki} \text{ by renaming variables in the second sum} \\
2Tr(\mathbf{AB}) &= 0 \\
Tr(\mathbf{AB}) &= 0 \\
\end{split}
\]

Since $\mathbf{A}$ and $\mathbf{B}$ were arbitrary, this holds for all pairs of symmetric and antisymmetric matrices 
of the same dimensionality.

\item 

Consider an arbitrary antisymmetric $N \times N$ matrix $\mathbf{B}$ and an arbitrary $N \times 1$ column vector $\mathbf{x}$.

\[
\begin{split}
\mathbf{x}^T\mathbf{A}\mathbf{x} &= \sum^n_{i=1}\sum^n_{k=1} x^T_{1i}a_{ik}x_{k1} \\
2\mathbf{x}^T\mathbf{A}\mathbf{x} &= \sum^n_{i=1}\sum^n_{k=1} x^T_{1i}a_{ik}x_{k1} + \sum^n_{i=1}\sum^n_{k=1} x^T_{1i}a_{ik}x_{k1} \\
2\mathbf{x}^T\mathbf{A}\mathbf{x} &= \sum^n_{i=1}\sum^n_{k=1} x^T_{1i}a_{ik}x_{k1} - \sum^n_{i=1}\sum^n_{k=1} x_{k1}a_{ki}x^T_{1i} \text{ by rearranging and since } \forall i, k \in \mathbb{N}. b_{ik} = -b_{ki} \\
2\mathbf{x}^T\mathbf{A}\mathbf{x} &= \sum^n_{i=1}\sum^n_{k=1} x^T_{1i}a_{ik}x_{k1} - \sum^n_{i=1}\sum^n_{k=1} x^T_{1k}a_{ki}x_{i1} \text{ by the definition of transpose}\\
2\mathbf{x}^T\mathbf{A}\mathbf{x} &= \sum^n_{i=1}\sum^n_{k=1} x^T_{1i}a_{ik}x_{k1} - \sum^n_{i=1}\sum^n_{k=1} x^T_{1i}a_{ik}x_{k1} \text{ renaming variables in the second sum}\\
2\mathbf{x}^T\mathbf{A}\mathbf{x} &= 0 \\
\mathbf{x}^T\mathbf{A}\mathbf{x} &= 0 \\
\end{split}
\]

\end{enumerate}

\item $a_{ji}$

I will prove associativity for multiplying a vector by the result of a matrix multiplication: $\mathbf{A}(\mathbf{Bx}) = (\mathbf{AB})\mathbf{x}$
\begin{equation}\label{eq:associative}
\begin{split}
((ab)x)_{i} &= \sum^n_{j=1} (ab)_{ij}x_j \\
            &= \sum^n_{j=1} \left(\sum^n_{k=1} a_{ik}b_{kj}\right)x_j \\
            &= \sum^n_{j=1} \sum^n_{k=1} a_{ik}b_{kj}x_j \\
            &= \sum^n_{k=1} a_{ik} \sum^n_{j=1} b_{kj}x_j \\
            &= \sum^n_{k=1} a_{ik} (bx)_k \\
            &= (a(bx))_i \\
\end{split}
\end{equation}

\begin{enumerate}

\item $\mathbf{ABx}$

\item $\mathbf{Bx}$

\item $\mathbf{B}\mathbf{A}^T\mathbf{x}$

\item $\mathbf{A}\mathbf{A}^T\mathbf{A}$

\end{enumerate}

\item 

\begin{enumerate}

\item 

\[
\begin{pmatrix} 0 & 1 \\ 0 & 0 \\ \end{pmatrix} \begin{pmatrix} 0 & 1 \\ 0 & 0 \\ \end{pmatrix} = \begin{pmatrix} 0 & 0 \\ 0 & 0 \\ \end{pmatrix}
\]

\item 

Consider:

\begin{align*}
\mathbf{A} &= \begin{pmatrix} 0 & 1 \\ 0 & 0 \\ \end{pmatrix} & \mathbf{B} &= \begin{pmatrix} 1 & 1 \\ 0 & 0 \\ \end{pmatrix}
\end{align*}

\[
\mathbf{BA} = \begin{pmatrix} 0 & 1 \\ 0 & 0 \\ \end{pmatrix}
\]

\[
\mathbf{AB} = \begin{pmatrix} 0 & 0 \\ 0 & 0 \\ \end{pmatrix}
\]

\item 

\[
\begin{pmatrix} 1 & 0 \\ 0 & -1 \\ \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & 0 \\ \end{pmatrix} = \begin{pmatrix} 1 & 1 \\ -1 & 0 \\ \end{pmatrix}
\]

\end{enumerate}

\item 

\[
\mathbf{M}^3 = -\mathbf{M} \Longrightarrow \mathbf{M}^{2n} = (-1)^{n + 1}\mathbf{M}^2 \wedge \mathbf{M}^{2n + 1} = (-1)^{n}\mathbf{M}
\]

\[
\begin{split}
\exp\theta\mathbf{M} &= \mathbf{I} + \sum^\infty_{n=1}\frac{(\theta\mathbf{M})^n}{n!} \\
                           &= \mathbf{I} + \sum^\infty_{n=0}\frac{(\theta\mathbf{M})^{2n + 1}}{(2n + 1)!} + \sum^\infty_{n=1}\frac{(\theta\mathbf{M})^{2n}}{(2n)!} \\
                           &= \mathbf{I} + \mathbf{M}\sum^\infty_{n=0}\frac{(-1)^n\theta^{2n + 1}}{(2n + 1)!} + \mathbf{M}^2\sum^\infty_{n=1}\frac{(-1)^{n + 1}\theta^{2n}}{(2n)!} \\
                           &= \mathbf{I} + \mathbf{M}\sum^\infty_{n=0}\frac{(-1)^n\theta^{2n + 1}}{(2n + 1)!} + \mathbf{M}^2\sum^\infty_{n=0}\frac{(-1)^{n + 1}\theta^{2n}}{(2n)!} + \mathbf{M}^2 \\
                           &= \mathbf{I} + \mathbf{M}\sin\theta - \mathbf{M}^2\cos\theta + \mathbf{M}^2 \text{ using the Taylor expansion for } \sin\theta \text{ and } \cos\theta \\
                           &= \mathbf{I} + \mathbf{M}\sin\theta + \mathbf{M}^2(1 - \cos\theta) \\
\end{split}
\]

\[
\begin{split}
\exp\theta_1\mathbf{M}\exp\theta_2\mathbf{M} &= \left(\mathbf{I} + \mathbf{M}\sin\theta_1 + \mathbf{M}^2(1 - \cos\theta_1)\right)\left(\mathbf{I} + \mathbf{M}\sin\theta_2 + \mathbf{M}^2(1 - \cos\theta_2)\right) \\
                                                         &= \mathbf{I} + \mathbf{M}(\sin\theta_1 + \sin\theta_2) + \mathbf{M}^2(2 - \cos\theta_1 - \cos\theta_2) + \mathbf{M}^2\sin\theta_1\sin\theta_2 \\
                                                         & \ \ \ \ \ \ + \mathbf{M}^3(\sin\theta_1 + \sin\theta_2 - 2\sin\theta_1\cos\theta_2) + \mathbf{M}^4(1 - \cos\theta_1 - \cos\theta_2 + \cos\theta_1\cos\theta_2)\\
                                                         &= \mathbf{I} + \mathbf{M}(\sin\theta_1 + \sin\theta_2) + \mathbf{M}^2(2 + \sin\theta_1\sin\theta_2 - \cos\theta_1 - \cos\theta_2) \\
                                                         & \ \ \ \ \ \ - \mathbf{M}(\sin\theta_1 + \sin\theta_2 - 2\sin\theta_1\cos\theta_2) - \mathbf{M}^4(1 - \cos\theta_1 - \cos\theta_2 + \cos\theta_1\cos\theta_2)\\
                                                         &= \mathbf{I} + \mathbf{M}(\sin\theta_1 + \sin\theta_2 - \sin\theta_1 - \sin\theta_2 + 2\sin\theta_1\cos\theta_2) \\
                                                         & \ \ \ \ \ \ + \mathbf{M}^2(2 + \sin\theta_1\sin\theta_2 - \cos\theta_1 - \cos\theta_2 - 1 + \cos\theta_1 + \cos\theta_2 - \cos\theta_1\cos\theta_2)\\
                                                         &= \mathbf{I} + \mathbf{M}(2\sin\theta_1\cos\theta_2) + \mathbf{M}^2(1 + \sin\theta_1\sin\theta_2 - \cos\theta_1\cos\theta_2) \\
                                                         &= \mathbf{I} + \mathbf{M}\sin(\theta_1 +\theta_2) + \mathbf{M}^2(1 - \cos(\theta_1 + \theta_2)) \\
                                                         &= \exp(\theta_1 + \theta_2)\mathbf{M} \\
\end{split}
\]

\[
\begin{split}
\exp\theta\mathbf{M} &= \mathbf{I}^T + \mathbf{M}^T\sin\theta + \left(\mathbf{M}^2\right)^T(1 - \cos\theta) \\
                           &= \mathbf{I} - \mathbf{M}\sin\theta + \mathbf{M}^2(1 - \cos\theta) \\
                           &= \exp(-\theta)\mathbf{M} \\
\end{split}
\]

\[
\begin{split}
(\exp\theta\mathbf{M})(\exp\theta\mathbf{M})^T &= \exp\theta\mathbf{M}\exp(-\theta)\mathbf{M} \\
                                                           &= \exp(\theta - \theta)\mathbf{M} \\
                                                           &= \exp0\mathbf{M} \\
                                                           &= \mathbf{I} + \mathbf{M}\sin 0 + \mathbf{M}^2(1 - \cos 0)\\
                                                           &= \mathbf{I} \\
\end{split}
\]

This result will not hold for general $\mathbf{M}$. $\mathbf{M}$ has the special property that
$\mathbf{M}^3 = -\mathbf{M}$ .
This holds for antisymmetric matrices with exactly one non-zero skew diagonal.
I couldn't find the name of this type of matrix.

\item 

\begin{align*}
\mathbf{b} &= \begin{pmatrix} b_1 \\ b_2 \\ b_3 \\ \end{pmatrix} & \mathbf{B} &= \begin{pmatrix} B_{11} & B_{12} & B_{13} \\ B_{21} & B_{22} & B_{23} \\ B_{31} & B_{32} & B_{33} \\ \end{pmatrix}
\end{align*}

Consider arbitrary
\[
\mathbf{x} = \begin{pmatrix} x \\ y \\ z \\ \end{pmatrix}
\]

\[
\begin{split}
\mathbf{b} \times \mathbf{x} &= \mathbf{B}\mathbf{x} \\
\begin{pmatrix} b_2 z - b_3 y \\ b_3 x - b_1 z \\ b_1 y - b_2 x \\ \end{pmatrix} &= \begin{pmatrix} B_{11}x + B_{12}y +
B_{13}z \\ B_{21}x + B_{22}y + B_{23}z \\ B_{31}x + B_{32}y + B_{33}z \\ \end{pmatrix}
\end{split}
\]
Equating coefficients of $x$, $y$ and $z$ gives:
\begin{align*}
B_{11} &= 0 & B_{12} &= -b_3 & B_{13} &= b_2 \\
B_{21} &= b_3 & B_{22} &= 0 & B_{23} &= -b_1 \\
B_{31} &= -b_2 & B_{32} &= b_1 & B_{33} &= 0 \\
\end{align*}

So:
\[
\mathbf{B} = \begin{pmatrix} 0 & -b_3 & b_2 \\ b_3 & 0 & -b_1 \\ -b_2 & b_1 & 0 \\ \end{pmatrix}
\]

\[
\begin{split}
\mathbf{B}^2\mathbf{x} &= \mathbf{B}(\mathbf{B}\mathbf{x})~\text{ using (\ref{eq:associative})}\\
                       &= \mathbf{b} \times (\mathbf{b} \times \mathbf{x}) \\
\end{split}
\]

\[
\begin{split}
\mathbf{B}^2\mathbf{x} &= \begin{pmatrix} -b_2^2 - b_3^2 & b_1 b_2 & b_1 b_3 \\ b_1 b_2 & -b_1^2 - b_3^2 & b_2 b_3 \\
b_1 b_3 & b_2 b_3 & -b_1^2 - b_2^2 \\ \end{pmatrix} \begin{pmatrix} x \\ y \\ z \\ \end{pmatrix} \\
                       &= \begin{pmatrix} -b_2^2 x - b_3^2 x + b_1 b_2 y + b_1 b_3 z \\ b_1 b_2 x - b_1^2 y - b_3^2 y +
                       b_2 b_3 z \\ b_1 b_3 x + b_2 b_3 y - b_1^2 z - b_2^2 z \\ \end{pmatrix} \\
                       &= \begin{pmatrix} (b_1 x + b_2 y + b_3 z)b_1 \\ (b_1 x + b_2 y + b_3 z)b_2 \\ (b_1 x + b_2 y +
                       b_3 z)
                       b_3 \\ \end{pmatrix} - \begin{pmatrix} (b_1^2 + b_2^2 + b_3^2)x \\ (b_1^2 + b_2^2 + b_3^2)y \\ (b_1^2 + b_2^2 + b_3^2)z \\ \end{pmatrix} \\
                       &= (\mathbf{b}\cdot\mathbf{x})\mathbf{b} - (\mathbf{b}\cdot\mathbf{b})\mathbf{x} \\
\end{split}
\]

Equating the two equations gives the required result:
\[
\mathbf{b} \times (\mathbf{b} \times \mathbf{x}) = (\mathbf{b}\cdot\mathbf{x})\mathbf{b} - (\mathbf{b}\cdot\mathbf{b})\mathbf{x} \\
\]

\item 

\[
\begin{split}
\det{\mathbf{A}}\det{\mathbf{B}} &= (4 - 6) \times (4 - 4) \\
                                 &= -2 \times 0 \\
                                 &= 0 \\
\end{split}
\]

\[
\begin{split}
\det{\mathbf{AB}} &= \det{\begin{pmatrix} 4 & 8 \\ 10 & 20 \\ \end{pmatrix}} \\
                  &= 80 - 80 \\
                  &= 0 \\
                  &= \det{\mathbf{A}}\det{\mathbf{B}} \\
\end{split}
\]

\[
\begin{split}
\det{\mathbf{A}^{-1}} &= \det{\begin{pmatrix} -2 & 1 \\ \frac{3}{2} & -\frac{1}{2} \\ \end{pmatrix}} \\
                      &= 1 - \frac{3}{2} \\
                      &= -\frac{1}{2} \\
                      &= \frac{1}{\det{\mathbf{A}}} \\
\end{split}
\]

\item 

\[
\begin{split}
\begin{vmatrix}  1 & 2 & 3 \\ 3 & 0 & 1 \\ 1 & 0 & 1 \\ \end{vmatrix} &= A_{11}C_{1,1} + A_{12}C_{1,2} + A_{13}C_{1,3} \\
                                                                      &= 1 \begin{vmatrix} 0 & 1 \\ 0 & 1 \\ \end{vmatrix} - 2 \begin{vmatrix} 3 & 1 \\ 1 & 1 \\ \end{vmatrix} + 3 \begin{vmatrix} 3 & 0 \\ 1 & 0 \\ \end{vmatrix} \\
                                                                      &= 0 - 4 + 0 \\ 
                                                                      &= -4 \\
\end{split}
\]

\[
\begin{split}
\begin{vmatrix}
1 & 2 & 3 \\ 
3 & 0 & 1 \\ 
1 & 0 & 1 \\ 
\end{vmatrix}
&= 
\begin{vmatrix}
-2 & 2 & 3 \\ 
2 & 0 & 1 \\ 
0 & 0 & 1 \\ 
\end{vmatrix} \\
&= 
\begin{vmatrix}
-2 & 2 & 3 \\ 
0 & 2 & 4 \\ 
0 & 0 & 1 \\ 
\end{vmatrix} \\
&= -4 \\
\end{split}
\]

\[
\begin{split}
|\mathbf{A}^T|
&=
\begin{vmatrix}
1 & 2 & 3 \\ 
3 & 0 & 1 \\ 
1 & 0 & 1 \\ 
\end{vmatrix}^T \\
&= 
\begin{vmatrix}
1 & 3 & 1 \\ 
2 & 0 & 0 \\ 
3 & 1 & 1 \\ 
\end{vmatrix} \\
&= 
\begin{vmatrix}
-2 & 3 & 1 \\ 
2 & 0 & 0 \\ 
0 & 1 & 1 \\ 
\end{vmatrix} \\
&= 
\begin{vmatrix}
-2 & 3 & 1 \\ 
0 & 3 & 1 \\ 
0 & 1 & 1 \\ 
\end{vmatrix} \\
&= 
\begin{vmatrix}
-2 & 2 & 1 \\ 
0 & 2 & 1 \\ 
0 & 0 & 1 \\ 
\end{vmatrix} \\
&= -2 \times 2 \times 1 \\
&= -4 \\
&= |\mathbf{A}|
\end{split}
\]

\item 

The columns of a matrix who's determinant is zero are linearly dependent on each other.

The vectors 
\begin{align*}
\begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \\ \end{pmatrix} & & \begin{pmatrix} 2 \\ 1 \\ 3 \\ 2 \\ \end{pmatrix} & & \begin{pmatrix} 4 \\ 0 \\ 1 \\ 3 \\ \end{pmatrix} & & \begin{pmatrix} 2 \\ 0 \\ 3 \\ a \\ \end{pmatrix} \\
\end{align*}
are linearly dependent if and only if the determinant of the matrix with these vectors as it's columns is zero.

\[
\begin{split}
\begin{vmatrix}
1 & 0 & 1 & 0 \\
2 & 1 & 3 & 2 \\
4 & 0 & 1 & 3 \\
2 & 0 & 3 & a \\
\end{vmatrix}
&= 
\begin{vmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
4 & 0 & 1 & 3 \\
2 & 0 & 3 & a \\
\end{vmatrix} \\
&= 
\begin{vmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
4 & 0 & -3 & 3 \\
2 & 0 & 1 & a \\
\end{vmatrix} \\
&= 
\begin{vmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & -3 & 3 \\
0 & 0 & 1 & a \\
\end{vmatrix} \\
&= 
\begin{vmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & -3 & 3 \\
0 & 0 & 0 & a + 1 \\
\end{vmatrix} \\
&= -3(a + 1) \\
\end{split}
\]

So the vectors are linearly dependent for $a = -1$.

\item 

We can use row operations to conserve the determinant of the matrix:
\[
\begin{split}
\begin{vmatrix}
a - E & -b & -b & -b & -b \\
-b & a - E & -b & -b & -b \\
-b & -b & a - E & -b & -b \\
-b & -b & -b & a - E & -b \\
-b & -b & -b & -b & a - E \\
\end{vmatrix}
&= 
\begin{vmatrix}
a + b - E & 0 & 0 & 0 & -b \\
0 & a + b - E & 0 & 0 & -b \\
0 & 0 & a + b - E & 0 & -b \\
0 & 0 & 0 & a + b - E & -b \\
E - a - b & E - a - b & E - a - b & E - a - b & a - E \\
\end{vmatrix} \\
&= 
\begin{vmatrix}
a + b - E & 0 & 0 & 0 & -b \\
0 & a + b - E & 0 & 0 & -b \\
0 & 0 & a + b - E & 0 & -b \\
0 & 0 & 0 & a + b - E & -b \\
0 & 0 & 0 & 0 & a - 4b - E \\
\end{vmatrix} \\
&= (a + b - E)^4(a - 4b - E) \\
\end{split}
\]

So the determinant is zero when:
\[
\begin{split}
(a + b - E)^4(a - 4b - E) &= 0 \\
E = a + b &\vee E = a - 4b \\
\end{split}
\]

\item 

\[
\begin{pmatrix} 1 & 0 & 1 & 0 \\ 2 & 1 & 0 & 1 \\ 0 & 1 & 0 & 3 \\ 1 & 3 & 2 & 0 \\ \end{pmatrix} \begin{pmatrix} p \\ y \\ z \\ w \\ \end{pmatrix} = \begin{pmatrix} 2 \\ 4 \\ 3 \\ 6 \\ \end{pmatrix}
\]

\item 

\[
\begin{split}
4w &= 8 \\
w &= 2 \\
\end{split}
\]

\[
\begin{split}
2z + w &= 4 \\
2z + 2 &= 2 \\
2z &= 2 \\
z &= 1 \\
\end{split}
\]

\[
\begin{split}
2y + 3z + 5w &= 19 \\
2y + 3 + 10 &= 19 \\
2y &= 6 \\
y &= 3 \\
\end{split}
\]

\[
\begin{split}
6x + 7y + 3z - w &= -2 \\
6x + 21 + 3 - 2 &= -2 \\
6x &= -24 \\
x &= -4 \\
\end{split}
\]

\begin{align*}
x &= -4 & y &= 3 & z &= 1 & w &= 2
\end{align*}

\item 

\[
\begin{split}
x + 3y &= 17 \\
3x + 9y &= 51 \\
(3x + 9y) - (3x + 2y) &= 51 - 9 \\
7y &= 42 \\
y &= 6 \\
\end{split}
\]
\[
\begin{split}
x + 3y &= 17 \\
x + 18 &= 17 \\
x &= -1 \\
\end{split}
\]
\begin{align*}
x &= -1 & y &= 6
\end{align*}

\[
\begin{split}
\begin{pmatrix} 3 & 2 \\ 1 & 3 \\ \end{pmatrix} \begin{pmatrix} x \\ y \\ \end{pmatrix} &= \begin{pmatrix} 9 \\ 17 \\ \end{pmatrix} \\
\begin{pmatrix} x \\ y \\ \end{pmatrix} &= \frac{1}{7}\begin{pmatrix} 3 & -2 \\ -1 & 3 \\ \end{pmatrix} \begin{pmatrix} 9 \\ 17 \\ \end{pmatrix} \\
\begin{pmatrix} x \\ y \\ \end{pmatrix} &= \begin{pmatrix} -1 \\ 6 \\ \end{pmatrix} \\
\end{split}
\]

\item 

\[
\begin{split}
\mathbf{A}^{-1}
&= 
\frac{1}{|\mathbf{A}|}
\begin{pmatrix}
C_{1, 1} & C_{1, 2} & C_{1, 3} \\
C_{2, 1} & C_{2, 2} & C_{2, 3} \\
C_{3, 1} & C_{3, 2} & C_{3, 3} \\
\end{pmatrix}^T \\
&= 
\frac{1}{4}
\begin{pmatrix}
1 & 1 & 2 \\ 
-2 & 2 & 0 \\
-1 & -1 & 2 \\
\end{pmatrix}^T \\
&= \frac{1}{4}
\begin{pmatrix}
1 & -2 & -1 \\ 
1 & 2 & -1 \\
2 & 0 & 2 \\
\end{pmatrix} \\
\end{split}
\]

\[
\begin{split}
\mathbf{A}\mathbf{x} &= \mathbf{y} \\
\mathbf{x} &= \mathbf{A}^{-1}\mathbf{y} \\
\mathbf{x} &= 
\frac{1}{4}
\begin{pmatrix}
1 & -2 & -1 \\ 
1 & 2 & -1 \\
2 & 0 & 2 \\
\end{pmatrix} 
\begin{pmatrix}
a \\ b \\ c \\
\end{pmatrix} \\
&= 
\frac{1}{4}
\begin{pmatrix}
a - 2b - c \\
a + 2b - c \\
2a + 2c \\
\end{pmatrix}
\end{split}
\]

\begin{align*}
\mathbf{e}_1 &= \frac{1}{4}\begin{pmatrix} 1 \\ 1 \\ 2 \\ \end{pmatrix} & \mathbf{e}_2 &= \frac{1}{4}\begin{pmatrix} -2 \\ 2 \\ 0 \\ \end{pmatrix} & \mathbf{e}_3 &= \frac{1}{4}\begin{pmatrix} -1 \\ -1 \\ 2 \\ \end{pmatrix} 
\end{align*}

The matrix whose columns are $\mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3$ is the inverse matrix of $\mathbf{A}$.

\item

\[
a_i\cdot b_j = \delta_{ij}
\]

Consider $a_i \cdot b_i$:

Case $i = j$:

\[
\begin{split}
a_j \cdot b_i &= 1 \\
              &= \frac{[a_1, a_2, a_3]}{[a_1, a_2, a_3]} \\
              &= \frac{a_i \cdot a_{(i + 1) \% 3} \times a_{(i + 2) \% 3}}{[a_1, a_2, a_3]} \\
              &= a_j \cdot \frac{a_{(i + 1)\% 3}\times a_{(i + 2) \% 3}}{[a_1, a_2, a_3]} \\
\end{split}
\]

Case $i \neq j$:

\[
\begin{split}
a_j \cdot b_i &= 0 \\
              &= \frac{a_j \cdot a_{(i + 1) \% 3} \times a_{(i + 2) \% 3}}{[a_1, a_2, a_3]} \\
              &= a_j \cdot \frac{a_{(i + 1) \% 3} \times a_{(i + 2) \% 3}}{[a_1, a_2, a_3]} \\
\end{split}
\]

\[
\begin{split}
\forall \mathbf{A}, \mathbf{B} \in \mathbb{R}^{3 \times 3}. \mathbf{AB} &= \mathbf{I} \Longrightarrow \\
\forall i, j \in \{1,2,3\}. a_j \cdot b_i &= a_j \cdot \frac{a_{(i + 1)\% 3}\times a_{(i + 2) \% 3}}{[a_1, a_2, a_3]} \Longrightarrow \\
\forall i, j \in \{1, 2, 3\}. b_i &= \frac{a_{(i + 1)\% 3}\times a_{(i + 2) \% 3}}{[a_1, a_2, a_3]} \\
\end{split}
\]

Substituting $i = 1, 2, 3$ into these formulae gives the required result:
\begin{align*}
b_1 &= \frac{a_2 \times a_3}{[a_1, a_2, a_3]} & b_2 &= \frac{a_3 \times a_1}{[a_1, a_2, a_3]} & b_3 &= \frac{a_1 \times a_2}{[a_1, a_2, a_3]}
\end{align*}

\end{enumerate}

\end{document}